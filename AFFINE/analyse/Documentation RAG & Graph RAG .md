# Documentation RAG & Graph RAG 



**Version**: PROTOCOLE DAN v5

**Audience**: DÃ©veloppeurs, Product Owners, Architectes

**DerniÃ¨re mise Ã  jour**: Novembre 2025

***

## Table des matiÃ¨res

1. [Vue d'ensemble](#1-vue-densemble)
2. [Architecture du systÃ¨me RAG](#2-architecture-du-systÃ¨me-rag)
3. [Pipeline d'ingestion des documents](#3-pipeline-dingestion-des-documents)
4. [PROTOCOLE DAN v5 - Agent ReAct](#4-protocole-dan-v5---agent-react)
5. [StratÃ©gies de recherche (Legacy)](#5-stratÃ©gies-de-recherche-legacy)
6. [Graph RAG - Exploitation du graphe de connaissances](#6-graph-rag---exploitation-du-graphe-de-connaissances)
7. [Configuration et paramÃ©trage](#7-configuration-et-paramÃ©trage)
8. [Exemples d'utilisation](#8-exemples-dutilisation)
9. [MÃ©triques et monitoring](#9-mÃ©triques-et-monitoring)
10. [Pistes d'Ã©volution et roadmap](#10-pistes-dÃ©volution-et-roadmap)

***

## 1. Vue d'ensemble

### 1.1 Qu'est-ce qu'un RAG ?

**RAG (Retrieval-Augmented Generation)** est une architecture qui combine :

* **Retrieval** (Recherche) : Trouver les documents/passages pertinents dans une base de connaissances
* **Augmentation** : Enrichir le contexte avec ces informations
* **Generation** : GÃ©nÃ©rer une rÃ©ponse via un LLM en utilisant ce contexte enrichi

### 1.2 Notre approche : Graph RAG Hybride

ChatDocAI utilise une approche **hybride innovante** qui combine :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GRAPH RAG HYBRIDE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  ğŸ” RECHERCHE VECTORIELLE    +    ğŸ•¸ï¸ GRAPHE DE              â”‚
â”‚     (SimilaritÃ© sÃ©mantique)        CONNAISSANCES            â”‚
â”‚                                    (Relations entre entitÃ©s) â”‚
â”‚                                                              â”‚
â”‚  ğŸ“Š FULL-TEXT SEARCH         +    ğŸ§  AGENT ReAct            â”‚
â”‚     (Recherche lexicale)           (Raisonnement cognitif)  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantages de cette architecture** :

* âœ… **PrÃ©cision** : Combine la recherche sÃ©mantique et lexicale
* âœ… **Contexte** : Exploite les relations entre entitÃ©s (Graph RAG)
* âœ… **Intelligence** : Agent capable de raisonner et rÃ©soudre les corÃ©fÃ©rences
* âœ… **TraÃ§abilitÃ©** : Citations prÃ©cises avec rÃ©fÃ©rences aux documents sources
* âœ… **FlexibilitÃ©** : Multiples stratÃ©gies selon le type de question

### 1.3 Stack technique

| Composant                  | Technologie                           | RÃ´le                                      |
| -------------------------- | ------------------------------------- | ----------------------------------------- |
| **Base de donnÃ©es graphe** | Neo4j                                 | Stockage des vecteurs, entitÃ©s, relations |
| **Embeddings**             | OpenAI (text-embedding-3-small/large) | Conversion texte â†’ vecteurs               |
| **LLM Extraction**         | gpt-4.1-mini-2025-04-14               | Extraction d'entitÃ©s et relations         |
| **LLM Planning**           | gpt-4.1-nano-2025-04-14               | Raisonnement et planification             |
| **LLM Synthesis**          | gpt-4.1-2025-04-14                    | GÃ©nÃ©ration de rÃ©ponses                    |
| **Parsing**                | Docling + PyMuPDF                     | Extraction de texte multi-format          |
| **Storage**                | MinIO + Supabase                      | Documents bruts + mÃ©tadonnÃ©es             |

***

## 2. Architecture du systÃ¨me RAG

### 2.1 SchÃ©ma d'architecture global

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER QUESTION                             â”‚
â”‚                  "Quelle est la franchise aggravÃ©e ?"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROTOCOLE DAN v5                              â”‚
â”‚                   (Agent ReAct)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  [1] REASON ğŸ§                                                    â”‚
â”‚      â”œâ”€ Analyse la question                                     â”‚
â”‚      â”œâ”€ RÃ©sout les corÃ©fÃ©rences (historique conversationnel)    â”‚
â”‚      â”œâ”€ Identifie les synonymes juridiques                      â”‚
â”‚      â””â”€ Formule une requÃªte optimisÃ©e                           â”‚
â”‚           â†“                                                      â”‚
â”‚  [2] ACT ğŸ¯                                                      â”‚
â”‚      â”œâ”€ Recherche VECTORIELLE (embedding â†’ Neo4j)               â”‚
â”‚      â”œâ”€ Recherche FULL-TEXT (mots-clÃ©s â†’ Lucene)                â”‚
â”‚      â”œâ”€ FUSION des rÃ©sultats (dÃ©duplication)                    â”‚
â”‚      â””â”€ RERANKING intelligent (LLM Ã©value pertinence)           â”‚
â”‚           â†“                                                      â”‚
â”‚  [3] OBSERVE ğŸ“                                                  â”‚
â”‚      â”œâ”€ SynthÃ¨se de la rÃ©ponse (gpt-4.1)                        â”‚
â”‚      â”œâ”€ Extraction des citations [Passage X]                    â”‚
â”‚      â””â”€ Formatage final                                         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI RESPONSE + CITATIONS                       â”‚
â”‚  "La franchise aggravÃ©e est un dispositif... [Passage 1]"       â”‚
â”‚                                                                  â”‚
â”‚  Sources: 3 passages sources â€¢ 2 documents                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Composants clÃ©s

#### 2.2.1 Service RAG (`backend/src/services/notaria_rag_service.py`)

**ResponsabilitÃ©s** :

* Orchestration de l'agent ReAct
* GÃ©nÃ©ration d'embeddings
* Extraction d'entitÃ©s via LLM
* SynthÃ¨se des rÃ©ponses avec citations
* Gestion de la mÃ©moire conversationnelle

**Points d'entrÃ©e principaux** :

```python
# PROTOCOLE DAN v5 - MÃ©thode principale
async def query(question: str, conversation_history: Optional[List]) -> Dict[str, Any]

# Legacy - Avec mÃ©triques
async def query_with_metrics(question: str) -> Dict[str, Any]

# Ingestion
async def ingest_raw_document(document_id: str, file_path: str, text_content: str) -> bool
```

#### 2.2.2 Service Neo4j (`backend/src/services/neo4j_service.py`)

**ResponsabilitÃ©s** :

* Gestion de la connexion Neo4j
* Indexation vectorielle (cosine similarity)
* Indexation full-text (Lucene)
* RequÃªtes Cypher pour le graphe
* Stockage des entitÃ©s et relations

**MÃ©thodes clÃ©s** :

```python
# Recherche vectorielle
async def search_chunks_by_vector(embedding: List[float], limit: int) -> List[Dict]

# Recherche full-text
async def search_chunks_by_fulltext(query: str, limit: int) -> List[Dict]

# Exploration du graphe
async def find_paths_between_entities(entity_names: List[str], max_depth: int) -> List[str]

# Enrichissement contextuel
async def get_relations_from_chunks(chunk_ids: List[str], limit: int) -> List[Dict]

# Visualisation
async def get_knowledge_graph(limit: int) -> Dict[str, Any]
```

***

## 3. Pipeline d'ingestion des documents

### 3.1 Vue d'ensemble du pipeline

Le pipeline transforme un document brut en une base de connaissances structurÃ©e et interrogeable.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document PDF â”‚
â”‚   (MinIO)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 1: PARSING                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ PyMuPDF (rapide) pour PDFs textuels                      â”‚
â”‚ â€¢ Docling + OCR pour PDFs scannÃ©s ou autres formats       â”‚
â”‚ â€¢ Support: PDF, DOCX, TXT, MD, EML, HTML, PPT, XLS, etc.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Texte extrait
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 2: CHUNKING SÃ‰MANTIQUE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ DÃ©coupage en chunks de 512 tokens                        â”‚
â”‚ â€¢ Respect des paragraphes comme unitÃ©s sÃ©mantiques         â”‚
â”‚ â€¢ Overlap de 50 tokens pour la cohÃ©rence                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Liste de chunks
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 3: ANALYSE SÃ‰MANTIQUE (LLM)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Extraction d'entitÃ©s (Personnes, Orgs, Concepts, etc.)  â”‚
â”‚ â€¢ Extraction de relations (EST_UN_TYPE_DE, etc.)          â”‚
â”‚ â€¢ Large chunks (60K tokens) pour l'analyse globale        â”‚
â”‚ â€¢ LLM: gpt-4.1-mini avec prompt spÃ©cialisÃ© notarial       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ EntitÃ©s + Relations          â”‚ Chunks
             â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 4a: EMBEDDING      â”‚    â”‚ Ã‰TAPE 4b: GRAPH STORAGE  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ text-embedding-3-small â”‚    â”‚ â€¢ CrÃ©ation des nÅ“uds     â”‚
â”‚ â€¢ 1536 dimensions        â”‚    â”‚   Entity, Document       â”‚
â”‚ â€¢ Batch processing       â”‚    â”‚ â€¢ CrÃ©ation des relations â”‚
â”‚ â€¢ OpenAI API             â”‚    â”‚ â€¢ Labels typÃ©s           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                               â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 5: INDEXATION NEO4J                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Index vectoriel (cosine similarity)                      â”‚
â”‚ â€¢ Index full-text (Lucene)                                 â”‚
â”‚ â€¢ NÅ“uds: Chunk â†’ Document                                  â”‚
â”‚ â€¢ Relations: BELONGS_TO, MENTIONED_IN, etc.                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Code d'ingestion simplifiÃ©

**Fichier**: `backend/scripts/ingestion_pipeline.py`

```python
# Traitement parallÃ¨le avec concurrence contrÃ´lÃ©e
CONCURRENCY_LIMIT = 2  # Nombre de documents traitÃ©s simultanÃ©ment

# Parsing adaptatif selon le format
if file_extension == '.eml':
    text, metadata = self._parse_eml_file(filename, content)
elif file_extension == '.pdf':
    # Tentative rapide avec PyMuPDF
    text = extract_with_pymupdf(content)
    if len(text) < MIN_CONTENT_LENGTH:
        # Fallback sur Docling avec OCR
        text = self._extract_text_with_docling(filename, content)
else:
    # Autres formats â†’ Docling directement
    text = self._extract_text_with_docling(filename, content)

# Envoi au service RAG pour analyse complÃ¨te
success = await rag_service.ingest_raw_document(
    document_id=filename,
    file_path=filename,
    text_content=text
)
```

### 3.3 StratÃ©gie de chunking

Le chunking est **sÃ©mantiquement conscient** :

```python
# 1. Respect des paragraphes
paragraphs = text.split('\n\n')

# 2. Segmentation par phrases
sentences = re.split(r'(?<=[.!?])\s+', paragraph)

# 3. AgrÃ©gation jusqu'Ã  512 tokens
current_chunk_tokens = []
for tokens in sentence_tokens:
    if len(current_chunk_tokens) + len(tokens) > 512:
        # Finaliser le chunk
        chunks.append(decode(current_chunk_tokens))

        # 4. Overlap de 50 tokens pour cohÃ©rence
        overlap_start = max(0, len(current_chunk_tokens) - 50)
        current_chunk_tokens = current_chunk_tokens[overlap_start:]

    current_chunk_tokens.extend(tokens)
```

**Pourquoi 512 tokens ?**

* âœ… Suffisamment large pour capturer le contexte
* âœ… Assez petit pour prÃ©cision de la recherche
* âœ… Compatible avec les limites d'embedding
* âœ… Ã‰quilibre performance/coÃ»t

### 3.4 Extraction d'entitÃ©s et relations

Le LLM reÃ§oit un **prompt spÃ©cialisÃ© notarial** :

```
Tu es un expert en modÃ©lisation de connaissance pour le domaine notarial franÃ§ais.
Extrais TOUTES les entitÃ©s et relations pertinentes.

TYPES D'ENTITÃ‰S :
- Personne: "Me Romain Lecordier", "Eric Dupond-Moretti"
- Organisation: "MMA IARD", "Conseil SupÃ©rieur du Notariat"
- ConceptJuridique: "franchise aggravÃ©e", "sociÃ©tÃ© multi-offices (SMO)"
- Document: "contrat nÂ° 145 154 406", "dÃ©cret 2024-906"
- Date: "1er janvier 2025"
- Lieu: "Basse-Normandie", "Caen"

TYPES DE RELATIONS :
- EST_UN_TYPE_DE
- S_APPLIQUE_A
- A_POUR_REGLE
- MEMBRE_DE
- SITUÃ‰_Ã€
- A_POUR_DATE_DEFFET
```

**RÃ©sultat** : Un graphe de connaissances structurÃ© dans Neo4j.

***

## 4. PROTOCOLE DAN v5 - Agent ReAct

### 4.1 Qu'est-ce que ReAct ?

**ReAct** = **Rea**soning + **Act**ing

Architecture cognitive qui simule le raisonnement humain :

1. **Penser** (Reason) â†’ Analyser le problÃ¨me
2. **Agir** (Act) â†’ ExÃ©cuter des actions
3. **Observer** (Observe) â†’ Ã‰valuer les rÃ©sultats

### 4.2 ImplÃ©mentation dans ChatDocAI

```python
async def query(question: str, conversation_history: Optional[List]) -> Dict:
    """
    PROTOCOLE DAN v5 - Agent ReAct avec mÃ©moire conversationnelle
    """

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ã‰TAPE 1: RAISONNEMENT (REASON)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    thought_process = await self._reasoning_step(question, conversation_history)
    search_query = thought_process["search_query"]

    # L'agent analyse:
    # âœ“ Le contexte conversationnel (10 derniers messages)
    # âœ“ Les corÃ©fÃ©rences ("cette nÃ©gociation" â†’ sujet rÃ©el)
    # âœ“ Les synonymes juridiques ("recours" â†’ "rÃ©clamation, mÃ©diateur")
    # âœ“ Les termes exacts Ã  rechercher

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ã‰TAPE 2: ACTION (ACT)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    context_chunks = await self._hybrid_search_step(search_query)

    # L'agent exÃ©cute:
    # âœ“ Recherche vectorielle (similaritÃ© sÃ©mantique)
    # âœ“ Recherche full-text (mots-clÃ©s exacts)
    # âœ“ Fusion et dÃ©duplication
    # âœ“ Reranking intelligent (LLM Ã©value la pertinence)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ã‰TAPE 3: OBSERVATION & SYNTHÃˆSE (OBSERVE)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    return await self._synthesize_answer_with_citations(question, context_chunks)

    # L'agent gÃ©nÃ¨re:
    # âœ“ RÃ©ponse nuancÃ©e basÃ©e sur les chunks
    # âœ“ Citations prÃ©cises [Passage X]
    # âœ“ Gestion de l'incertitude
```

### 4.3 Ã‰tape REASON - RÃ©solution de corÃ©fÃ©rences

**Exemple concret** :

```
Historique conversationnel:
  User: "Quelles sont les rÃ¨gles pour une nÃ©gociation immobiliÃ¨re ?"
  AI: "Une nÃ©gociation immobiliÃ¨re nÃ©cessite un mandat Ã©crit..."

Question actuelle:
  User: "Y a-t-il des limites ?"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT REASONING (LLM nano)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Thought:                                                â”‚
â”‚ "L'utilisateur demande 'y a-t-il des limites' mais     â”‚
â”‚  ne prÃ©cise pas de quoi. En analysant l'historique,    â”‚
â”‚  le sujet est 'nÃ©gociation immobiliÃ¨re'. Je dois donc   â”‚
â”‚  rÃ©soudre la corÃ©fÃ©rence."                              â”‚
â”‚                                                         â”‚
â”‚ Search Query:                                           â”‚
â”‚ "limites nÃ©gociation immobiliÃ¨re notaire dÃ©ontologie   â”‚
â”‚  mandat durÃ©e montant"                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.4 Ã‰tape ACT - Recherche hybride

```python
async def _hybrid_search_step(search_query: str) -> List[Dict]:
    # 1. GÃ©nÃ©ration de l'embedding
    vector_embedding = await self._get_embeddings([search_query])

    # 2. Recherche parallÃ¨le
    vector_results, fulltext_results = await asyncio.gather(
        neo4j.search_chunks_by_vector(vector_embedding[0], limit=7),
        neo4j.search_chunks_by_fulltext(search_query, limit=7)
    )

    # 3. Fusion et dÃ©duplication
    all_chunks = {chunk['chunkId']: chunk for chunk in vector_results}
    for chunk in fulltext_results:
        if chunk['chunkId'] not in all_chunks:
            all_chunks[chunk['chunkId']] = chunk

    # 4. Reranking LLM
    reranked = await self._rerank_chunks(search_query, list(all_chunks.values()))

    # 5. SÃ©lection finale
    return reranked[:5]  # Top 5 chunks
```

**Pourquoi le reranking ?**

La similaritÃ© vectorielle ne garantit pas la pertinence rÃ©elle. Le LLM Ã©value :

* Le chunk contient-il **vraiment** la rÃ©ponse ?
* Quelle est la **qualitÃ©** de l'information ?
* Le contexte est-il **complet** ?

```python
# Prompt de reranking
"""
Pour chaque passage, Ã©value sa pertinence sur une Ã©chelle de 0 Ã  10:
- 10: Contient directement la rÃ©ponse
- 7-9: TrÃ¨s pertinent et liÃ©
- 4-6: Contexte partiel
- 1-3: Faiblement liÃ©
- 0: Non pertinent

Question: "Quelle est la franchise aggravÃ©e ?"

Passages:
--- Passage 0 ---
La franchise aggravÃ©e est un dispositif qui porte la franchise Ã 
5 fois son montant initial en cas de sinistre de mauvaise foi...

--- Passage 1 ---
Le contrat d'assurance MMA IARD couvre les risques cyber...

RÃ©ponse: {"scores": [{"passage": 0, "score": 10}, {"passage": 1, "score": 2}]}
```

### 4.5 Ã‰tape OBSERVE - SynthÃ¨se avec citations

Le prompt de synthÃ¨se intÃ¨gre une **gestion nuancÃ©e de l'incertitude** :

```
Tu es un assistant notarial expert. RÃ©ponds en te basant EXCLUSIVEMENT
sur les passages fournis.

DIRECTIVES :

1. RÃ©ponse claire â†’ Fournis-la avec citations [Passage X]

2. RÃ©ponse partielle â†’ Ne dis PAS "information non disponible"
   - Fournis ce que tu as trouvÃ©
   - Indique ce qui manque
   - Invite Ã  prÃ©ciser
   Exemple: "Les documents mentionnent les franchises pour la fraude
   informatique [Passage 2], mais ne spÃ©cifient pas l'option 8.
   Pourriez-vous prÃ©ciser le type de contrat ?"

3. Aucune information â†’ Formule nuancÃ©e
   "Je n'ai pas trouvÃ© d'information prÃ©cise. Serait-il possible de
   reformuler ou d'apporter plus de dÃ©tails ?"

PASSAGES EXTRAITS:
[Passage 1 - Document: contrat_mma.pdf]
La franchise aggravÃ©e est un dispositif...
```

**Extraction automatique des citations** :

```python
# Regex pour trouver les [Passage X]
passage_indices = re.findall(r'\[Passage (\d+)\]', answer)

# Construction de la liste de citations
for index in unique_indices:
    chunk = context_chunks[index]
    citations.append({
        "documentId": chunk['documentId'],
        "documentPath": chunk['documentPath'],
        "text": chunk['text']
    })

# Nettoyage de la rÃ©ponse
cleaned_answer = re.sub(r'\s*\[Passage \d+\]', '', answer)
```

***

## 5. StratÃ©gies de recherche (Legacy)

**Note**: Le PROTOCOLE DAN v5 est la mÃ©thode principale. Les stratÃ©gies ci-dessous restent disponibles via `query_with_metrics()` pour le monitoring.

### 5.1 Les trois stratÃ©gies

```python
class RAGStrategy(Enum):
    VECTOR_ONLY = "VECTOR_ONLY"    # Questions conceptuelles
    GRAPH_FIRST = "GRAPH_FIRST"    # Questions relationnelles
    HYBRID = "HYBRID"               # Questions complexes (dÃ©faut)
```

### 5.2 Query Planner (LLM)

Un LLM rapide (`gpt-4.1-nano`) choisit la stratÃ©gie optimale :

```python
async def _llm_query_planner(question: str) -> RAGStrategy:
    prompt = """
    DÃ©termine la meilleure stratÃ©gie parmi :

    - "VECTOR_ONLY": Questions gÃ©nÃ©rales, conceptuelles
      Ex: "Qu'est-ce qu'une SMO ?"

    - "GRAPH_FIRST": Questions sur des relations entre entitÃ©s
      Ex: "Quel est le lien entre MMA IARD et le contrat Cyber ?"

    - "HYBRID": Questions complexes (dÃ©faut)
      Ex: "Quelle est la procÃ©dure si un cohÃ©ritier ne rÃ©pond pas ?"

    Question: "{question}"

    RÃ©ponds en JSON: {"strategy": "HYBRID"}
    """
    # LLM retourne la stratÃ©gie optimale
```

**Fallback regex** si le LLM Ã©choue :

```python
# DÃ©tection de questions relationnelles
if re.search(r'\b(qui|lien|relation|associÃ©)\b', question):
    return RAGStrategy.GRAPH_FIRST

# DÃ©tection de questions conceptuelles
if re.search(r'\b(explique|dÃ©finition|qu\'est-ce que)\b', question):
    return RAGStrategy.VECTOR_ONLY

# DÃ©faut
return RAGStrategy.HYBRID
```

### 5.3 VECTOR\_ONLY - Recherche pure

```python
async def _execute_vector_only(question: str) -> str:
    # 1. Embedding de la question
    question_embedding = await self._get_embeddings([question])

    # 2. Recherche Neo4j
    chunks = await neo4j.search_chunks_by_vector(
        question_embedding[0],
        limit=10
    )

    # 3. Construction du contexte
    return "\n".join([chunk['text'] for chunk in chunks])
```

**Use case** : "Explique le principe de la franchise aggravÃ©e"

### 5.4 GRAPH\_FIRST - Exploration relationnelle

```python
async def _execute_graph_first(question: str) -> str:
    # 1. Extraction des entitÃ©s de la question
    entities = await self._extract_entities_from_query(question)
    # Ex: ["MMA IARD", "contrat Cyber"]

    # 2. Recherche de chemins dans le graphe
    paths = await neo4j.find_paths_between_entities(entities, max_depth=3)

    # 3. Formatage du contexte
    context = "Informations du graphe:\n"
    for path in paths:
        context += f"- {path}\n"

    return context
```

**RequÃªte Neo4j sous-jacente** :

```cypher
MATCH (e1:Entity), (e2:Entity)
WHERE e1.nom IN ['MMA IARD', 'contrat Cyber']
  AND e2.nom IN ['MMA IARD', 'contrat Cyber']
  AND elementId(e1) < elementId(e2)
MATCH p = allShortestPaths((e1)-[*1..3]-(e2))
RETURN p
LIMIT 20
```

**RÃ©sultat** : Chemins comme `'MMA IARD' --[FOURNIT]--> 'contrat Cyber'`

### 5.5 HYBRID - Approche combinÃ©e

```python
async def _execute_hybrid(question: str) -> str:
    # 1. Recherche vectorielle (5 chunks)
    chunks = await neo4j.search_chunks_by_vector(embedding, limit=5)

    # 2. Enrichissement par le graphe
    chunk_ids = [c['chunkId'] for c in chunks]
    relations = await neo4j.get_relations_from_chunks(chunk_ids, limit=10)

    # 3. Contexte fusionnÃ©
    context = "Contexte des documents:\n"
    for chunk in chunks:
        context += f"- {chunk['text']}\n"

    context += "\nInformations du graphe:\n"
    for rel in relations:
        context += f"- '{rel['entite']}' {rel['relation']} '{rel['autre_entite']}'\n"

    return context
```

**Parcours du graphe** :

```cypher
// Partir des chunks pertinents
MATCH (chunk:Chunk) WHERE chunk.id IN $chunk_ids

// Remonter au document
MATCH (chunk)-[:BELONGS_TO]->(doc:Document)

// Trouver les entitÃ©s mentionnÃ©es
MATCH (entity1:Entity)-[:MENTIONED_IN]->(doc)

// Explorer les relations
MATCH (entity1)-[relation]-(entity2:Entity)
WHERE entity1 <> entity2

RETURN entity1.nom, type(relation), entity2.nom
```

***

## 6. Graph RAG - Exploitation du graphe de connaissances

### 6.1 ModÃ¨le de donnÃ©es Neo4j

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document    â”‚
â”‚  {id, path}  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ BELONGS_TO
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Chunk        â”‚
â”‚  {id, text,      â”‚
â”‚   embedding}     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Entity       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚     Entity       â”‚
â”‚  {nom, type,     â”‚ RELATIONâ”‚  {nom, type,     â”‚
â”‚   description}   â”‚         â”‚   description}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ MENTIONED_IN
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Document    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Types de nÅ“uds

| Label      | Description       | PropriÃ©tÃ©s clÃ©s                         |
| ---------- | ----------------- | --------------------------------------- |
| `Document` | Document source   | `id`, `filePath`, `created_at`          |
| `Chunk`    | Fragment de texte | `id`, `text`, `embedding`, `documentId` |
| `Entity`   | EntitÃ© extraite   | `nom`, `type`, `description`            |

### 6.3 Types de relations

| Relation         | Description       | Exemple                                  |
| ---------------- | ----------------- | ---------------------------------------- |
| `BELONGS_TO`     | Chunk â†’ Document  | chunk\_0 â†’ doc\_contract.pdf             |
| `MENTIONED_IN`   | Entity â†’ Document | "MMA IARD" â†’ doc\_contract.pdf           |
| `EST_UN_TYPE_DE` | Taxonomie         | "franchise aggravÃ©e" â†’ "franchise"       |
| `S_APPLIQUE_A`   | Application       | "dÃ©cret 2024-906" â†’ "inspection"         |
| `MEMBRE_DE`      | Appartenance      | "Me Lecordier" â†’ "Chambre Notaires"      |
| `A_POUR_REGLE`   | RÃ¨gle             | "contrat cyber" â†’ "mot de passe 12 char" |

### 6.4 RequÃªtes utiles pour le PO

#### Visualiser le graphe complet

```cypher
MATCH (e:Entity)-[r]-(e2:Entity)
RETURN e, r, e2
LIMIT 300
```

#### Trouver les entitÃ©s les plus connectÃ©es

```cypher
MATCH (e:Entity)-[r]-()
RETURN e.nom, e.type, count(r) as connections
ORDER BY connections DESC
LIMIT 20
```

#### Explorer les relations d'une entitÃ©

```cypher
MATCH (e:Entity {nom: "MMA IARD"})-[r]-(other:Entity)
RETURN e.nom, type(r), other.nom, other.type
```

#### Statistiques globales

```cypher
MATCH (e:Entity)
RETURN e.type as EntityType, count(*) as Count
ORDER BY Count DESC
```

***

## 7. Configuration et paramÃ©trage

### 7.1 Variables d'environnement clÃ©s

```shellscript
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# EMBEDDINGS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMBEDDING_MODEL=text-embedding-3-small    # ou text-embedding-3-large
EMBEDDING_DIMENSIONS=1536                 # 1536 ou 3072

# âš ï¸ Changer EMBEDDING_DIMENSIONS nÃ©cessite une rÃ©indexation complÃ¨te

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM ENDPOINTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM_EXTRACTION_MODEL=gpt-4.1-mini-2025-04-14   # Extraction d'entitÃ©s
LLM_PLANNER_MODEL=gpt-4.1-nano-2025-04-14      # Raisonnement agent
LLM_SYNTHESIS_MODEL=gpt-4.1-2025-04-14         # GÃ©nÃ©ration rÃ©ponse

LLM_EXTRACTION_TEMPERATURE=0.0    # DÃ©terministe
LLM_PLANNER_TEMPERATURE=0.0       # DÃ©terministe
LLM_SYNTHESIS_TEMPERATURE=0.3     # LÃ©gÃ¨rement crÃ©atif

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CHUNKING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RETRIEVAL_CHUNK_SIZE_TOKENS=512   # Taille des chunks pour recherche
RETRIEVAL_OVERLAP_TOKENS=50       # Overlap entre chunks

ANALYSIS_CHUNK_SIZE_TOKENS=60000  # Taille pour analyse LLM
EMBEDDING_TOKEN_LIMIT_PER_CHUNK=8190
EMBEDDING_BATCH_TOKEN_LIMIT=250000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NEO4J
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NEO4J_URL=bolt://your-neo4j:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_password
```

### 7.2 Tuning des performances

#### Augmenter la qualitÃ© des embeddings

```shellscript
# Passer Ã  text-embedding-3-large SANS rÃ©indexation
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIMENSIONS=1536  # RÃ©duction Ã  1536 (compatible)

# âœ… Meilleure qualitÃ© sans coÃ»t de rÃ©indexation
```

#### RÃ©duire les coÃ»ts

```shellscript
# RÃ©duire la taille des chunks d'analyse
ANALYSIS_CHUNK_SIZE_TOKENS=30000  # Au lieu de 60000

# Utiliser des modÃ¨les moins chers
LLM_EXTRACTION_MODEL=gpt-4.1-nano-2025-04-14  # Au lieu de mini
```

#### AmÃ©liorer la prÃ©cision

```shellscript
# Augmenter la taille des chunks de retrieval
RETRIEVAL_CHUNK_SIZE_TOKENS=768   # Au lieu de 512

# Augmenter l'overlap
RETRIEVAL_OVERLAP_TOKENS=100      # Au lieu de 50
```

### 7.3 Commandes d'administration

```shellscript
# Ingestion complÃ¨te
python scripts/ingestion_pipeline.py

# Ingestion avec limite
python scripts/ingestion_pipeline.py --limit 10

# Forcer le retraitement
python scripts/ingestion_pipeline.py --force

# Purger et recommencer
python scripts/ingestion_pipeline.py --purge

# Mode dÃ©mon (surveillance continue)
python scripts/ingestion_pipeline.py --daemon
```

***

## 8. Exemples d'utilisation

### 8.1 Question conceptuelle simple

**Input** :

```json
{
  "question": "Qu'est-ce qu'une franchise aggravÃ©e ?",
  "conversation_history": []
}
```

**Traitement** :

```
REASON: Question conceptuelle, pas d'entitÃ©s spÃ©cifiques
        â†’ Query: "franchise aggravÃ©e dÃ©finition assurance"

ACT:    Vector search + Full-text
        â†’ 7 chunks vectoriels + 7 chunks full-text
        â†’ Fusion â†’ 10 chunks uniques
        â†’ Reranking â†’ Top 5

OBSERVE: SynthÃ¨se avec citations
```

**Output** :

```json
{
  "answer": "La franchise aggravÃ©e est un dispositif qui porte la franchise Ã  5 fois son montant initial en cas de sinistre rÃ©sultant d'une faute intentionnelle ou d'une nÃ©gligence grave de l'assurÃ©.",
  "citations": [
    {
      "documentId": "contrat_mma_pdf",
      "documentPath": "contrats/assurance/contrat_mma.pdf",
      "text": "Article 12 - Franchise aggravÃ©e : En cas de sinistre..."
    }
  ]
}
```

### 8.2 Question avec corÃ©fÃ©rence

**Input** :

```json
{
  "question": "Y a-t-il des limites ?",
  "conversation_history": [
    {"role": "user", "content": "Quelles sont les rÃ¨gles pour une nÃ©gociation immobiliÃ¨re ?"},
    {"role": "assistant", "content": "Une nÃ©gociation immobiliÃ¨re nÃ©cessite..."}
  ]
}
```

**Traitement** :

```
REASON: CorÃ©fÃ©rence dÃ©tectÃ©e â†’ "limites" se rÃ©fÃ¨re Ã  "nÃ©gociation immobiliÃ¨re"
        RÃ©solution: "limites nÃ©gociation immobiliÃ¨re notaire"
        â†’ Query enrichie avec synonymes juridiques

ACT:    Recherche hybride optimisÃ©e

OBSERVE: RÃ©ponse contextuelle
```

**Output** :

```json
{
  "answer": "Oui, la nÃ©gociation immobiliÃ¨re est encadrÃ©e par plusieurs limites. Le mandat ne peut excÃ©der 3 mois renouvelables, et le montant de la commission doit Ãªtre fixÃ© par Ã©crit.",
  "citations": [...]
}
```

### 8.3 Question relationnelle (Graph RAG)

**Input** :

```json
{
  "question": "Quel est le lien entre MMA IARD et le contrat cyber ?",
  "conversation_history": []
}
```

**Traitement** :

```
REASON: Question relationnelle â†’ 2 entitÃ©s identifiÃ©es
        EntitÃ©s: ["MMA IARD", "contrat cyber"]

ACT:    Graph traversal dans Neo4j
        â†’ Chemins trouvÃ©s entre les entitÃ©s
        â†’ Relations: FOURNIT, COUVRE, etc.

OBSERVE: SynthÃ¨se des relations
```

**Output** :

```json
{
  "answer": "MMA IARD est l'assureur qui fournit le contrat cyber pour couvrir les risques informatiques. Ce contrat s'applique aux Ã©tudes notariales et couvre notamment les fraudes informatiques, les ransomwares et les pertes de donnÃ©es.",
  "citations": [...]
}
```

### 8.4 RÃ©ponse partielle (incertitude)

**Input** :

```json
{
  "question": "Quelle est la franchise pour l'option 8 du contrat ITT ?"
}
```

**Output** :

```json
{
  "answer": "Les documents mentionnent les franchises gÃ©nÃ©rales pour la fraude informatique, mais ne spÃ©cifient pas le cas de l'option 8 du contrat ITT. Pourriez-vous prÃ©ciser le type de contrat ou l'assureur concernÃ© ?",
  "citations": [
    {
      "documentPath": "contrats/assurance/franchises_generales.pdf",
      "text": "Tableau des franchises standard..."
    }
  ]
}
```

***

## 9. MÃ©triques et monitoring

### 9.1 MÃ©triques d'ingestion

```python
# Rapport gÃ©nÃ©rÃ© aprÃ¨s ingestion
{
  "documents_processed": 150,
  "total_time_seconds": 1250.5,
  "avg_time_per_doc": 8.3,
  "neo4j_stats": {
    "nodes": 45230,
    "documents": 150,
    "chunks": 12450,
    "relations": 8920
  }
}
```

### 9.2 MÃ©triques de requÃªte (Legacy)

```python
# Via query_with_metrics()
{
  "answer": "La franchise aggravÃ©e...",
  "strategy": "VECTOR_ONLY",
  "chunks_retrieved": 10,
  "relations_found": 0,
  "execution_time_ms": 1850
}
```

### 9.3 Monitoring recommandÃ©

**Supabase** :

* Table `document_ingestion_status` : Suivi des documents traitÃ©s
* Statuts: `processing`, `success`, `error`, `invalid`

**Neo4j** :

```cypher
// Statistiques globales
CALL apoc.meta.stats()

// QualitÃ© des embeddings
MATCH (c:Chunk)
WHERE c.embedding IS NOT NULL
RETURN count(c) as chunks_with_embeddings
```

**Logs applicatifs** :

```python
logger.info(f"  > RÃ©cupÃ©ration: {len(vector)} chunks (vecteur)")
logger.info(f"  > Reranking: Top 3 scores: {scores}")
logger.info(f"  > SynthÃ¨se: {len(citations)} citations extraites")
```

***

## 10. Pistes d'Ã©volution et roadmap

### 10.1 AmÃ©liorations court terme (1-3 mois)

#### 1. Cache intelligent des requÃªtes

**Besoin** : Ã‰viter de retraiter les questions identiques

**Solution** : Redis/Memcached avec TTL de 15 minutes

```python
# Pseudo-code
query_hash = hashlib.sha256(question.encode()).hexdigest()
cached = redis.get(f"rag:query:{query_hash}")
if cached:
    return cached
```

**Impact** : âš¡ -80% latence sur questions rÃ©pÃ©tÃ©es

#### 2. Feedback loop utilisateur

**Besoin** : AmÃ©liorer la qualitÃ© des rÃ©ponses via les retours

**Solution** : SystÃ¨me de scoring (ğŸ‘/ğŸ‘) dÃ©jÃ  en place

```sql
-- Table evaluations existante
SELECT question, answer, rating, feedback
FROM evaluations
WHERE rating < 3
ORDER BY created_at DESC
```

**Action** : Analyser les mauvaises rÃ©ponses pour ajuster les prompts

#### 3. Multi-hop reasoning

**Besoin** : Questions nÃ©cessitant plusieurs Ã©tapes de raisonnement

**Exemple** : "Si un contrat est signÃ© le 15 janvier, quelle est la date limite pour le recours ?"

```python
# Agent ReAct itÃ©ratif
while not answer_found and steps < max_steps:
    thought = await reason(question, context)
    action_result = await act(thought)
    if is_sufficient(action_result):
        answer_found = True
    else:
        context.append(action_result)
        steps += 1
```

**Impact** : ğŸ“ˆ +30% questions complexes rÃ©solues

### 10.2 AmÃ©liorations moyen terme (3-6 mois)

#### 4. Fine-tuning du modÃ¨le d'embedding

**Besoin** : Embeddings spÃ©cialisÃ©s pour le jargon notarial

**Solution** : Fine-tune OpenAI embeddings sur corpus notarial

```python
# CrÃ©er des paires (question, passage_pertinent)
training_data = [
    ("Qu'est-ce qu'une SMO ?", "Une sociÃ©tÃ© multi-offices..."),
    ("Franchise aggravÃ©e", "La franchise aggravÃ©e est..."),
    # ... 10K+ paires
]

# Fine-tuning via OpenAI API
openai.FineTuningJob.create(...)
```

**Impact** : ğŸ¯ +15% prÃ©cision recherche vectorielle

#### 5. Graph enrichment automatique

**Besoin** : InfÃ©rer de nouvelles relations

**Solution** : Agent d'enrichissement continu (dÃ©jÃ  en place mais Ã  amÃ©liorer)

```python
# backend/src/agents/graph_enrichment_agent.py
async def infer_transitive_relations():
    # Si A â†’ B et B â†’ C, alors A â†’ C ?
    query = """
    MATCH (a)-[:EST_UN_TYPE_DE]->(b)-[:EST_UN_TYPE_DE]->(c)
    WHERE NOT (a)-[:EST_UN_TYPE_DE]->(c)
    CREATE (a)-[:EST_UN_TYPE_DE {inferred: true}]->(c)
    """
```

**Impact** : ğŸ•¸ï¸ +40% relations exploitables

#### 6. Support multi-lingue

**Besoin** : Documents en anglais, espagnol, etc.

**Solution** : Embeddings multilingues + dÃ©tection de langue

```python
# DÃ©tection automatique
from langdetect import detect
lang = detect(text)

# ModÃ¨le multilingue
if lang != 'fr':
    embedding_model = "text-embedding-3-large-multilingual"
```

**Impact** : ğŸŒ Support international

### 10.3 AmÃ©liorations long terme (6-12 mois)

#### 7. RAG conversationnel avancÃ©

**Besoin** : Conversations multi-tours complexes

**Solution** : Memory augmentÃ©e avec rÃ©sumÃ©s intelligents

```python
# RÃ©sumÃ© automatique de l'historique
if len(conversation_history) > 20:
    summary = await llm.summarize(conversation_history[:10])
    context = summary + conversation_history[-10:]
```

**Impact** : ğŸ’¬ Conversations illimitÃ©es sans perte de contexte

#### 8. RAG multi-modal

**Besoin** : Extraire info des images, tableaux, graphiques

**Solution** : Vision models (GPT-4V, LLaVA) pour documents scannÃ©s

```python
# Extraction vision
if is_scanned_pdf(document):
    images = extract_images(document)
    for img in images:
        visual_content = await gpt4v.analyze(img)
        text_content += f"\n\n[Image: {visual_content}]"
```

**Impact** : ğŸ“Š +25% information extraite

#### 9. Benchmark automatique

**Besoin** : Mesurer la qualitÃ© du RAG objectivement

**Solution** : Dataset de test + mÃ©triques automatiques

```python
# backend/scripts/benchmark_quality_report.py (dÃ©jÃ  crÃ©Ã©)
test_cases = [
    {"question": "...", "expected_answer": "...", "must_cite": ["doc1.pdf"]},
    # ... 100+ cas
]

for case in test_cases:
    result = await rag.query(case["question"])
    score = evaluate(result, case["expected_answer"])
    metrics.append(score)

print(f"Accuracy: {sum(metrics)/len(metrics):.2%}")
```

**Impact** : ğŸ“ QualitÃ© mesurable et traÃ§able

#### 10. Optimisation des coÃ»ts LLM

**Besoin** : RÃ©duire les coÃ»ts OpenAI

**Solutions** :

* ModÃ¨les open-source (Llama 3, Mixtral) pour certaines tÃ¢ches
* Quantification et dÃ©ploiement local
* Cascade de modÃ¨les (nano â†’ mini â†’ full selon complexitÃ©)

```python
# Cascade intelligente
if is_simple_question(question):
    model = "gpt-4.1-nano"  # $0.15/1M tokens
elif is_moderate(question):
    model = "gpt-4.1-mini"  # $1.25/1M tokens
else:
    model = "gpt-4.1"       # $15/1M tokens
```

**Impact** : ğŸ’° -60% coÃ»ts API

### 10.4 Roadmap visuelle

```
Q1 2026
â”œâ”€ âœ… Cache intelligent
â”œâ”€ âœ… Feedback loop
â””â”€ âœ… Multi-hop reasoning

Q2 2026
â”œâ”€ ğŸ¯ Fine-tuning embeddings
â”œâ”€ ğŸ•¸ï¸ Graph enrichment auto
â””â”€ ğŸŒ Support multi-lingue

Q3-Q4 2026
â”œâ”€ ğŸ’¬ RAG conversationnel avancÃ©
â”œâ”€ ğŸ“Š RAG multi-modal (vision)
â”œâ”€ ğŸ“ Benchmark automatique
â””â”€ ğŸ’° Optimisation coÃ»ts
```

***

## Annexes

### A. Glossaire

| Terme               | DÃ©finition                                                      |
| ------------------- | --------------------------------------------------------------- |
| **Embedding**       | ReprÃ©sentation vectorielle d'un texte (ex: \[0.23, -0.45, ...]) |
| **Chunk**           | Fragment de texte (512 tokens par dÃ©faut)                       |
| **Entity**          | Ã‰lÃ©ment identifiÃ© (Personne, Org, Concept, etc.)                |
| **Relation**        | Lien entre deux entitÃ©s (ex: A --\[EST\_UN\_TYPE\_DE]--> B)     |
| **CorÃ©fÃ©rence**     | RÃ©fÃ©rence Ã  un Ã©lÃ©ment prÃ©cÃ©dent ("cette nÃ©gociation")          |
| **Reranking**       | RÃ©Ã©valuation de la pertinence par un LLM                        |
| **Graph traversal** | Exploration du graphe Neo4j                                     |
| **Cypher**          | Langage de requÃªte Neo4j (Ã©quivalent SQL)                       |

### B. RÃ©fÃ©rences

* [PROTOCOLE DAN v5 - Commit 8a1f5a3](https://github.com/Forgeai-platform/chatbot-CNCAC/commit/8a1f5a3)
* [Docling Documentation](https://github.com/DS4SD/docling)
* [Neo4j Vector Index](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/)
* [ReAct Paper](https://arxiv.org/abs/2210.03629)

### C. Contact et support

Pour toute question technique :

* **Code source** : `/backend/src/services/notaria_rag_service.py`
* **Tests** : `/backend/tests/test_notaria_rag_service.py`
* **Documentation architecture** : `CLAUDE.md`, `ARCHITECTURE.md`

***

**Document gÃ©nÃ©rÃ© le 2025-11-04**

**Auteur**: Claude Code

**Version**: 1.0
